{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d95ec17",
      "metadata": {
        "id": "6d95ec17"
      },
      "outputs": [],
      "source": [
        "# Cropping train and test images according to bounding boxes in digitStruct.mat\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "from PIL import Image\n",
        "from tqdm import tqdm  # voortgangsbalk\n",
        "\n",
        "\n",
        "def load_digit_struct(mat_file):\n",
        "    def _read_name(f, name_ref):\n",
        "        return ''.join(chr(c[0]) for c in f[name_ref][:])\n",
        "\n",
        "    def _read_attr_values(f, attr_ds):\n",
        "        vals = []\n",
        "        if attr_ds.shape[0] > 1:\n",
        "            for i in range(attr_ds.shape[0]):\n",
        "                vals.append(int(f[attr_ds[i][0]][()][0][0]))\n",
        "        else:\n",
        "            vals.append(int(attr_ds[()][0][0]))\n",
        "        return vals\n",
        "\n",
        "    out = []\n",
        "    with h5py.File(mat_file, \"r\") as f:\n",
        "        ds = f[\"digitStruct\"]\n",
        "        names = ds[\"name\"]\n",
        "        bboxes = ds[\"bbox\"]\n",
        "        n = len(names)\n",
        "        for i in range(n):\n",
        "            name = _read_name(f, names[i][0])\n",
        "            bbox_group = f[bboxes[i][0]]\n",
        "            bbox = {}\n",
        "            for key in [\"label\", \"left\", \"top\", \"width\", \"height\"]:\n",
        "                bbox[key] = _read_attr_values(f, bbox_group[key])\n",
        "            out.append({\"name\": name, \"bbox\": bbox})\n",
        "    return out\n",
        "\n",
        "\n",
        "def preprocess_split(split_root, out_root):\n",
        "    os.makedirs(out_root, exist_ok=True)\n",
        "    mat_file = os.path.join(split_root, \"digitStruct.mat\")\n",
        "    entries = load_digit_struct(mat_file)\n",
        "\n",
        "    for entry in tqdm(entries, desc=f\"Cropping {os.path.basename(split_root)}\"):\n",
        "        img_path = os.path.join(split_root, entry[\"name\"])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        for idx, (l, left, top, w, h) in enumerate(\n",
        "            zip(entry[\"bbox\"][\"label\"], entry[\"bbox\"][\"left\"], entry[\"bbox\"][\"top\"],\n",
        "                entry[\"bbox\"][\"width\"], entry[\"bbox\"][\"height\"])\n",
        "        ):\n",
        "            label = 0 if l == 10 else l  # in SVHN: label=10 betekent \"0\"\n",
        "            x1, y1, x2, y2 = int(left), int(top), int(left+w), int(top+h)\n",
        "            crop = img.crop((x1, y1, x2, y2))\n",
        "\n",
        "            label_dir = os.path.join(out_root, str(label))\n",
        "            os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "            # unieker bestandsnaam: originele naam + index\n",
        "            base_name = os.path.splitext(entry[\"name\"])[0]\n",
        "            crop_path = os.path.join(label_dir, f\"{base_name}_{idx}.png\")\n",
        "            crop.save(crop_path)\n",
        "\n",
        "\n",
        "# Crop train and test images\n",
        "SVHN_ROOT = \"\"\n",
        "OUT_ROOT = \"SVHN_crops\"\n",
        "\n",
        "preprocess_split(os.path.join(SVHN_ROOT, \"train\"), os.path.join(OUT_ROOT, \"train\"))\n",
        "\n",
        "preprocess_split(os.path.join(SVHN_ROOT, \"test\"), os.path.join(OUT_ROOT, \"test\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03818edb",
      "metadata": {
        "id": "03818edb",
        "outputId": "4adff8cf-9263-4d3c-aebe-1536007ed540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Train samples: 36843, Val samples: 4093, Test samples: 26032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\f_voe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[01/15] train loss 1.6104 acc 0.4671 | val loss 1.3518 acc 0.5314 | 43.7s\n",
            "  ✓ New best val acc: 0.5314 -> saved to svhn_cnn_best.pth\n",
            "[02/15] train loss 0.9667 acc 0.7175 | val loss 0.8044 acc 0.7652 | 29.4s\n",
            "  ✓ New best val acc: 0.7652 -> saved to svhn_cnn_best.pth\n",
            "[03/15] train loss 0.6927 acc 0.7994 | val loss 0.6202 acc 0.8199 | 28.6s\n",
            "  ✓ New best val acc: 0.8199 -> saved to svhn_cnn_best.pth\n",
            "[04/15] train loss 0.5714 acc 0.8333 | val loss 0.4957 acc 0.8537 | 28.7s\n",
            "  ✓ New best val acc: 0.8537 -> saved to svhn_cnn_best.pth\n",
            "[05/15] train loss 0.5076 acc 0.8494 | val loss 0.4807 acc 0.8480 | 28.5s\n",
            "[06/15] train loss 0.4668 acc 0.8598 | val loss 0.4427 acc 0.8664 | 28.3s\n",
            "  ✓ New best val acc: 0.8664 -> saved to svhn_cnn_best.pth\n",
            "[07/15] train loss 0.4347 acc 0.8705 | val loss 0.4608 acc 0.8515 | 28.4s\n",
            "[08/15] train loss 0.4109 acc 0.8759 | val loss 0.3929 acc 0.8776 | 29.4s\n",
            "  ✓ New best val acc: 0.8776 -> saved to svhn_cnn_best.pth\n",
            "[09/15] train loss 0.3954 acc 0.8815 | val loss 0.3922 acc 0.8808 | 48.9s\n",
            "  ✓ New best val acc: 0.8808 -> saved to svhn_cnn_best.pth\n",
            "[10/15] train loss 0.3771 acc 0.8881 | val loss 0.3798 acc 0.8871 | 49.8s\n",
            "  ✓ New best val acc: 0.8871 -> saved to svhn_cnn_best.pth\n",
            "[11/15] train loss 0.3707 acc 0.8876 | val loss 0.4089 acc 0.8644 | 48.4s\n",
            "[12/15] train loss 0.3593 acc 0.8919 | val loss 0.3643 acc 0.8920 | 47.7s\n",
            "  ✓ New best val acc: 0.8920 -> saved to svhn_cnn_best.pth\n",
            "[13/15] train loss 0.3492 acc 0.8950 | val loss 0.3531 acc 0.8949 | 47.8s\n",
            "  ✓ New best val acc: 0.8949 -> saved to svhn_cnn_best.pth\n",
            "[14/15] train loss 0.3383 acc 0.8980 | val loss 0.3501 acc 0.8881 | 48.0s\n",
            "[15/15] train loss 0.3325 acc 0.8992 | val loss 0.3743 acc 0.8842 | 47.5s\n",
            "TEST  loss 0.3526  acc 0.8953\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# SVHN single-digit training script\n",
        "# images have been cropped to single digits\n",
        "# using the bounding boxes provided in the dataset\n",
        "# and saved in  SVHN_crops/train and SVHN_crops/test\n",
        "# ========================\n",
        "import time, random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "\n",
        "# Set seed for reproducibility\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "seed_everything(42)\n",
        "\n",
        "#  Paths & Device\n",
        "device = \"cpu\"\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "train_dir = \"SVHN_crops/train\"\n",
        "test_dir  = \"SVHN_crops/test\"\n",
        "\n",
        "#  Transforms\n",
        "\n",
        "MEAN = [0.4377, 0.4438, 0.4728]\n",
        "STD  = [0.1980, 0.2010, 0.1970]\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "train_ds_0 = ImageFolder(root=train_dir, transform=train_tf)\n",
        "test_ds_0  = ImageFolder(root=test_dir, transform=val_tf)\n",
        "\n",
        "\n",
        "#  Model\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 16x16\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 8x8\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 4x4\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Training and evalutation loops\n",
        "def accuracy(logits, targets):\n",
        "    preds = logits.argmax(1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = labels.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += (logits.argmax(1) == labels).float().sum().item()\n",
        "        n += bs\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        bs = labels.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += (logits.argmax(1) == labels).float().sum().item()\n",
        "        n += bs\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "# Data & Training setup\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "VAL_SPLIT = 0.1\n",
        "EPOCHS = 15\n",
        "LR = 1e-3\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "full_train_ds = train_ds_0\n",
        "n_total = len(full_train_ds)\n",
        "n_val = int(VAL_SPLIT * n_total)\n",
        "n_train = n_total - n_val\n",
        "train_ds, val_ds = random_split(full_train_ds, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds_0, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\"\n",
        "      + (f\", Test samples: {len(test_loader.dataset)}\" if test_loader else \"\"))\n",
        "\n",
        "\n",
        "#  Model, loss, optimizer\n",
        "model = SmallCNN(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_path = \"svhn_cnn_best.pth\"\n",
        "\n",
        "# Train\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    t0 = time.time()\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"[{epoch:02d}/{EPOCHS}] \"\n",
        "          f\"train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
        "          f\"val loss {val_loss:.4f} acc {val_acc:.4f} | {dt:.1f}s\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"New best val acc: {best_val_acc:.4f} -> saved to {best_path}\")\n",
        "\n",
        "# Test\n",
        "if test_loader:\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "    print(f\"TEST  loss {test_loss:.4f}  acc {test_acc:.4f}\")\n",
        "else:\n",
        "    print(\"No test set available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "949bb840",
      "metadata": {
        "id": "949bb840",
        "outputId": "b0ab0400-f54e-4ad7-d0ba-e5ea5452a3cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Train samples: 36843, Val samples: 4093, Test samples: 26032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\f_voe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[16/30] train loss 0.3381 acc 0.8980 | val loss 0.3715 acc 0.8859 | 44.3s\n",
            "  ✓ New best val acc: 0.8859 -> saved to svhn_cnn_best.pth\n",
            "[17/30] train loss 0.3300 acc 0.9012 | val loss 0.3411 acc 0.8974 | 29.2s\n",
            "  ✓ New best val acc: 0.8974 -> saved to svhn_cnn_best.pth\n",
            "[18/30] train loss 0.3228 acc 0.9014 | val loss 0.3355 acc 0.8974 | 29.3s\n",
            "[19/30] train loss 0.3172 acc 0.9037 | val loss 0.3236 acc 0.9023 | 29.3s\n",
            "  ✓ New best val acc: 0.9023 -> saved to svhn_cnn_best.pth\n",
            "[20/30] train loss 0.3074 acc 0.9071 | val loss 0.3331 acc 0.8949 | 29.4s\n",
            "[21/30] train loss 0.3003 acc 0.9103 | val loss 0.3428 acc 0.9001 | 29.2s\n",
            "[22/30] train loss 0.2952 acc 0.9118 | val loss 0.3626 acc 0.8876 | 29.0s\n",
            "[23/30] train loss 0.2593 acc 0.9235 | val loss 0.3025 acc 0.9062 | 29.0s\n",
            "  ✓ New best val acc: 0.9062 -> saved to svhn_cnn_best.pth\n",
            "[24/30] train loss 0.2523 acc 0.9248 | val loss 0.3059 acc 0.9069 | 30.8s\n",
            "  ✓ New best val acc: 0.9069 -> saved to svhn_cnn_best.pth\n",
            "[25/30] train loss 0.2487 acc 0.9258 | val loss 0.3024 acc 0.9084 | 32.3s\n",
            "  ✓ New best val acc: 0.9084 -> saved to svhn_cnn_best.pth\n",
            "[26/30] train loss 0.2468 acc 0.9251 | val loss 0.3063 acc 0.9072 | 32.0s\n",
            "[27/30] train loss 0.2425 acc 0.9278 | val loss 0.3077 acc 0.9054 | 31.8s\n",
            "[28/30] train loss 0.2417 acc 0.9283 | val loss 0.3013 acc 0.9120 | 31.8s\n",
            "  ✓ New best val acc: 0.9120 -> saved to svhn_cnn_best.pth\n",
            "[29/30] train loss 0.2465 acc 0.9255 | val loss 0.2893 acc 0.9123 | 31.9s\n",
            "  ✓ New best val acc: 0.9123 -> saved to svhn_cnn_best.pth\n",
            "[30/30] train loss 0.2427 acc 0.9283 | val loss 0.3117 acc 0.9086 | 32.0s\n",
            "TEST  loss 0.3064  acc 0.9093\n"
          ]
        }
      ],
      "source": [
        "# Continue training single-digit recognition (continue from the script above, saved parameters in svhn_cnn_best.pth)\n",
        "\n",
        "# ========================\n",
        "# SVHN single-digit training script\n",
        "# ========================\n",
        "import os, math, time, random\n",
        "import h5py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "seed_everything(42)\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "train_dir = \"SVHN_crops/train\"\n",
        "test_dir  = \"SVHN_crops/test\"\n",
        "\n",
        "# Transforms to tensors\n",
        "\n",
        "MEAN = [0.4377, 0.4438, 0.4728]\n",
        "STD  = [0.1980, 0.2010, 0.1970]\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "train_ds_0 = ImageFolder(root=train_dir, transform=train_tf)\n",
        "test_ds_0  = ImageFolder(root=test_dir, transform=val_tf)\n",
        "\n",
        "\n",
        "# Model\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 16x16\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 8x8\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 4x4\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Training and evaluation loops\n",
        "def accuracy(logits, targets):\n",
        "    preds = logits.argmax(1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = labels.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += (logits.argmax(1) == labels).float().sum().item()\n",
        "        n += bs\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        bs = labels.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += (logits.argmax(1) == labels).float().sum().item()\n",
        "        n += bs\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "# Data & Training setup\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "VAL_SPLIT = 0.1\n",
        "START_EPOCH = 15\n",
        "EPOCHS = 15\n",
        "LR = 1e-3\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "full_train_ds = train_ds_0\n",
        "n_total = len(full_train_ds)\n",
        "n_val = int(VAL_SPLIT * n_total)\n",
        "n_train = n_total - n_val\n",
        "train_ds, val_ds = random_split(full_train_ds, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds_0, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\"\n",
        "      + (f\", Test samples: {len(test_loader.dataset)}\" if test_loader else \"\"))\n",
        "\n",
        "\n",
        "\n",
        "# Define model, loss, optimize\n",
        "model = SmallCNN(num_classes=10)\n",
        "model.load_state_dict(torch.load(\"svhn_cnn_best.pth\", map_location=device))\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_path = \"svhn_cnn_best.pth\"\n",
        "\n",
        "# Train\n",
        "for epoch in range(START_EPOCH+1, START_EPOCH + EPOCHS + 1):\n",
        "    t0 = time.time()\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"[{epoch:02d}/{START_EPOCH + EPOCHS}] \"\n",
        "          f\"train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
        "          f\"val loss {val_loss:.4f} acc {val_acc:.4f} | {dt:.1f}s\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"  ✓ New best val acc: {best_val_acc:.4f} -> saved to {best_path}\")\n",
        "\n",
        "# Test\n",
        "if test_loader:\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "    print(f\"TEST  loss {test_loss:.4f}  acc {test_acc:.4f}\")\n",
        "else:\n",
        "    print(\"No test set available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "702b50e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "702b50e4",
        "outputId": "57b888bc-a59c-4be0-a838-4a51ce455f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Gekopieerd: 2000 afbeeldingen naar /content/train_subset\n",
            "Aantal geldige samples: 2000\n",
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 138MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start epoch  1\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0005\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0005\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0005\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0005\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0007\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0005\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0005\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0005\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0005\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0004\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0001\n",
            "Epoch 1, Loss: 0.0002\n",
            "Epoch 1, Loss: 0.0003\n",
            "Epoch 1, Loss: 0.0004\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3453877433.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         detections = self.transform.postprocess(\n\u001b[1;32m    120\u001b[0m             \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mmatched_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_roi_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mclass_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_regression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, boxes, image_shapes)\u001b[0m\n\u001b[1;32m    312\u001b[0m             )\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         return _multiscale_roi_align(\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mx_filtered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36m_multiscale_roi_align\u001b[0;34m(x_filtered, boxes, output_size, sampling_ratio, scales, mapper)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mrois_per_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrois\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_in_level\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         result_idx_in_level = roi_align(\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mper_level_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mrois_per_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/ops/roi_align.py\u001b[0m in \u001b[0;36mroi_align\u001b[0;34m(input, boxes, output_size, spatial_scale, sampling_ratio, aligned)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_roi_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0m_assert_has_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     return torch.ops.torchvision.roi_align(\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_torchbind_op_overload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# make bounding boxed around digits in SVHN images\n",
        "# ========================\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "import torchvision.transforms as T\n",
        "import torch.optim as optim\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import h5py\n",
        "from PIL import Image\n",
        "import shutil\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "class SVHNDataset(Dataset):\n",
        "    def __init__(self, root, mat_file, transforms=None, only_boxes=True):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.only_boxes = only_boxes\n",
        "        self.mat_path = mat_file\n",
        "        self.f = h5py.File(self.mat_path, 'r')\n",
        "        self.digitStruct = self.f['digitStruct']\n",
        "        self.names = self.digitStruct['name']\n",
        "        self.bboxes = self.digitStruct['bbox']\n",
        "        self.length = self.names.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __del__(self):\n",
        "        try:\n",
        "            self.f.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def _get_name(self, name_ref):\n",
        "        name_ds = self.f[name_ref]\n",
        "        return \"\".join(chr(c[0]) for c in name_ds[()])\n",
        "\n",
        "    def _get_bbox(self, bbox_ref):\n",
        "        bbox_grp = self.f[bbox_ref]\n",
        "\n",
        "        def extract(field):\n",
        "            ds = bbox_grp[field]\n",
        "            if ds.shape[0] == 1:\n",
        "                return [ds[()][0][0]]\n",
        "            else:\n",
        "                return [self.f[ds[i][0]][()][0][0] for i in range(ds.shape[0])]\n",
        "\n",
        "        left   = extract(\"left\")\n",
        "        top    = extract(\"top\")\n",
        "        width  = extract(\"width\")\n",
        "        height = extract(\"height\")\n",
        "        label  = extract(\"label\")\n",
        "        return left, top, width, height, label\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        name_ref = self.names[idx][0]\n",
        "        bbox_ref = self.bboxes[idx][0]\n",
        "\n",
        "        img_name = self._get_name(name_ref)\n",
        "        left, top, width, height, label = self._get_bbox(bbox_ref)\n",
        "\n",
        "        boxes, labels = [], []\n",
        "        for l, t, w, h, lbl in zip(left, top, width, height, label):\n",
        "            boxes.append([float(l), float(t), float(l + w), float(t + h)])\n",
        "            if self.only_boxes:\n",
        "                labels.append(1)\n",
        "            else:\n",
        "                lbl = int(lbl)\n",
        "                labels.append(0 if lbl == 10 else lbl)\n",
        "\n",
        "        img_path = os.path.join(self.root, \"train_subset\", img_name)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
        "        }\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def get_bounding_boxes(num_classes=2, pretrained=False):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "def show_predictions(model, dataset, device, idx=0, score_thresh=0.5):\n",
        "    model.eval()\n",
        "    img, target = dataset[idx]\n",
        "    img_tensor = img.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "\n",
        "    # Zet terug naar numpy voor tekenen\n",
        "    img_np = img.permute(1, 2, 0).numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(6,6))\n",
        "    ax.imshow(img_np)\n",
        "\n",
        "    # Ground-truth in groen\n",
        "    for box in target[\"boxes\"]:\n",
        "        x1, y1, x2, y2 = box.tolist()\n",
        "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                             fill=False, color=\"green\", linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    # Predictions in rood\n",
        "    boxes = outputs[0][\"boxes\"]\n",
        "    scores = outputs[0][\"scores\"]\n",
        "\n",
        "    for box, score in zip(boxes, scores):\n",
        "        if score < score_thresh:\n",
        "            continue\n",
        "        x1, y1, x2, y2 = box.tolist()\n",
        "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                             fill=False, color=\"red\", linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, y1-5, f\"{score:.2f}\", color=\"red\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Dataset\n",
        "drive_train_dir = \"/content/drive/MyDrive/SVHN/train\"\n",
        "local_train_dir = \"/content/train_subset\"\n",
        "os.makedirs(local_train_dir, exist_ok=True)\n",
        "\n",
        "# Check if drive_train_dir is accessible\n",
        "if not os.path.exists(drive_train_dir):\n",
        "    print(f\"Error: Directory not found: {drive_train_dir}\")\n",
        "else:\n",
        "    all_files = sorted([f for f in os.listdir(drive_train_dir) if f.endswith('.png')])\n",
        "    subset_files = all_files[4000:6000]\n",
        "\n",
        "    for fname in subset_files:\n",
        "        src = os.path.join(drive_train_dir, fname)\n",
        "        dst = os.path.join(local_train_dir, fname)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "    print(f\"Gekopieerd: {len(subset_files)} afbeeldingen naar {local_train_dir}\")\n",
        "\n",
        "# Check if local_train_dir is populated\n",
        "if not os.listdir(local_train_dir):\n",
        "    print(f\"Error: No files copied to {local_train_dir}. Please check the drive access and file paths.\")\n",
        "else:\n",
        "    local_mat_file = \"/content/digitStruct.mat\"\n",
        "    drive_mat_file = \"/content/drive/MyDrive/SVHN/train/digitStruct.mat\" # Corrected path\n",
        "\n",
        "    # Check if drive_mat_file exists before copying\n",
        "    if not os.path.exists(drive_mat_file):\n",
        "        print(f\"Error: digitStruct.mat not found at {drive_mat_file}. Please ensure the file exists.\")\n",
        "    else:\n",
        "        !cp \"{drive_mat_file}\" \"{local_mat_file}\"\n",
        "\n",
        "        # Check if local_mat_file exists after copying\n",
        "        if not os.path.exists(local_mat_file):\n",
        "            print(f\"Error: Failed to copy digitStruct.mat to {local_mat_file}. Please check permissions or available space.\")\n",
        "        else:\n",
        "\n",
        "            local_imgs = set(os.listdir(\"/content/train_subset\"))\n",
        "\n",
        "            dataset = SVHNDataset(\n",
        "                root=\"/content\",\n",
        "                mat_file=local_mat_file, # Use the local path\n",
        "                transforms=T.ToTensor()\n",
        "            )\n",
        "\n",
        "            # Maak een lijst van indices uit digitStruct die we mogen gebruiken\n",
        "            valid_indices = []\n",
        "            for i in range(len(dataset)):\n",
        "                name_ref = dataset.names[i][0]\n",
        "                img_name = dataset._get_name(name_ref)\n",
        "                if img_name in local_imgs:\n",
        "                    valid_indices.append(i)\n",
        "\n",
        "            print(f\"Aantal geldige samples: {len(valid_indices)}\")\n",
        "\n",
        "            if not valid_indices:\n",
        "                 print(\"Error: No valid indices found. This might mean the image names in digitStruct.mat do not match the copied images.\")\n",
        "            else:\n",
        "\n",
        "                dataset_small = Subset(dataset, valid_indices)\n",
        "\n",
        "\n",
        "                data_loader = DataLoader(dataset_small, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "                # Model\n",
        "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "                print(\"Device:\", device)\n",
        "                model = get_bounding_boxes(num_classes=2, pretrained=False)\n",
        "                saved_detector_path = os.path.join(\"/content/drive/MyDrive/SVHN\", \"svhn_detector.pth\")\n",
        "\n",
        "                # Check if saved_detector_path exists before loading\n",
        "                if not os.path.exists(saved_detector_path):\n",
        "                    print(f\"Error: Model weights not found at {saved_detector_path}. Please ensure the file exists.\")\n",
        "                else:\n",
        "                    model.load_state_dict(torch.load(saved_detector_path, map_location=device))\n",
        "                    model.to(device)\n",
        "                    print(next(model.parameters()).device)\n",
        "                    optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "                    # Training loop\n",
        "                    num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"start epoch \", epoch+1)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, targets in data_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "        epoch_loss /= len(data_loader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "        # show_predictions(model, dataset, device, idx=np.random.randint(len(dataset_small)), score_thresh=0.5)\n",
        "\n",
        "save_path = \"/content/svhn_detector2.pth\"\n",
        "\n",
        "torch.save(model.state_dict(), save_path)\n",
        "!cp \"/content/svhn_detector2.pth\" \"/content/drive/MyDrive/SVHN/svhn_detector2.pth\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn9LlccnDBaG",
        "outputId": "0e064205-e3b0-4847-f03e-aaf2d4f94e68"
      },
      "id": "Mn9LlccnDBaG",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/SVHN/train\" | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e78zUBLBghOr",
        "outputId": "3669af6b-f838-4600-a44c-73bf3606b0bf"
      },
      "id": "e78zUBLBghOr",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000.png\n",
            "10001.png\n",
            "10002.png\n",
            "10003.png\n",
            "10004.png\n",
            "10005.png\n",
            "10006.png\n",
            "10007.png\n",
            "10008.png\n",
            "10009.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"/content/drive/Mijn Drive/SVHN/train/digitStruct.mat\"\n",
        "print(os.path.exists(path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKToXcSLDYOr",
        "outputId": "0423b5df-0f2d-414c-b577-8f7480e2f327"
      },
      "id": "ZKToXcSLDYOr",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3dfa668b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "3dfa668b",
        "outputId": "4e17ffce-6e35-4c48-d833-a218e3a29f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 170MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'svhn_detector.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2523117322.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bounding_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"svhn_detector.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'svhn_detector.pth'"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# compose models and predict house number: detection bounding boxes, crop and classification\n",
        "# ========================\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "# ----------------------\n",
        "# Helper to draw boxes on picture\n",
        "# ----------------------\n",
        "\n",
        "def draw_boxes(image, results):\n",
        "    image = image.copy()\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for r in results:\n",
        "        x1, y1, x2, y2 = r[\"box\"]\n",
        "        digit = r[\"digit\"]\n",
        "        score = r[\"score\"]\n",
        "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
        "        draw.text((x1, y1), f\"{digit} ({score:.2f})\", fill=\"red\")\n",
        "    return image\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Load detector, trained parameters loaded from file: svhn_detector.pth\n",
        "# ----------------------\n",
        "def get_bounding_boxes(num_classes=2):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "detector = get_bounding_boxes(num_classes=2)\n",
        "detector.load_state_dict(torch.load(\"svhn_detector.pth\", map_location=device))\n",
        "detector.to(device)\n",
        "detector.eval()\n",
        "\n",
        "# ----------------------\n",
        "# Load Classifier, trained parameters loaded from file: svhn_cnn_best.pth\n",
        "# ----------------------\n",
        "class SVHNClassifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 16x16\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 8x8\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 4x4\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "classifier = SVHNClassifier()\n",
        "classifier.load_state_dict(torch.load(\"svhn_cnn_best.pth\", map_location=device))\n",
        "classifier.to(device)\n",
        "classifier.eval()\n",
        "\n",
        "# ----------------------\n",
        "# Transform to tensor and normalize\n",
        "# ----------------------\n",
        "detector_transform = T.ToTensor()\n",
        "\n",
        "MEAN = [0.4377, 0.4438, 0.4728]\n",
        "STD  = [0.1980, 0.2010, 0.1970]\n",
        "classifier_transform = T.Compose([\n",
        "    T.Resize((32, 32)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(MEAN, STD)\n",
        "])\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Pipeline\n",
        "# ----------------------\n",
        "def recognize_number(image_path, score_thresh=0.5):\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = detector_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = detector(img_tensor)\n",
        "\n",
        "    boxes = outputs[0][\"boxes\"]\n",
        "    scores = outputs[0][\"scores\"]\n",
        "\n",
        "    results = []\n",
        "    for box, score in zip(boxes, scores):\n",
        "        if score < score_thresh:\n",
        "            continue\n",
        "\n",
        "        x1, y1, x2, y2 = map(int, box.tolist())\n",
        "        cropped = image.crop((x1, y1, x2, y2))\n",
        "\n",
        "        crop_tensor = classifier_transform(cropped).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = classifier(crop_tensor).argmax(dim=1).item()\n",
        "\n",
        "        results.append({\"digit\": pred, \"box\": (x1, y1, x2, y2), \"score\": float(score)})\n",
        "\n",
        "\n",
        "    results = sorted(results, key=lambda r: r[\"box\"][0])       # Sort left to right\n",
        "    number = \"\".join(str(r[\"digit\"]) for r in results)  # Join digits as string\n",
        "\n",
        "    return number, results\n",
        "\n",
        "\n",
        "\n",
        "image_to_test = \"extra/340.png\"\n",
        "\n",
        "if os.path.exists(image_to_test):\n",
        "    number, results = recognize_number(image_to_test, score_thresh=0.6)\n",
        "    print(\"Predicted house number:\", number)\n",
        "else:\n",
        "    print(f\"File not found: {image_to_test}\")\n",
        "\n",
        "img_with_boxes = draw_boxes(Image.open(image_to_test).convert(\"RGB\"), results)\n",
        "img_with_boxes.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Lyt0D16gBgUu",
        "outputId": "b1a39e64-27f0-4663-b455-f4f44f067ec2"
      },
      "id": "Lyt0D16gBgUu",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3595646193.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m     \"\"\"\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \"\"\"\n\u001b[0;32m--> 616\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}